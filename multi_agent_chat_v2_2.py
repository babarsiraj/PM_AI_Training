# -*- coding: utf-8 -*-
"""multi_agent_chat_v2_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqFM4v6PW4xEemhOFb8frtZ3tLgn9ti1

acting on first trigger word like tell me, name not going beyond that
show conversation and trace working
recall same problem
"""

import datetime
import numpy as np
# hybrid_assistant.py
from datetime import datetime

from typing import List, Dict
from sentence_transformers import SentenceTransformer, util

# -----------------------------
# Memory Agent
# -----------------------------
from datetime import datetime
import numpy as np

class MemoryAgent:
    def __init__(self, embedder):
        self.embedder = embedder
        # list of dicts: {content, source, time, embedding}
        self.knowledge = []

    def store_fact(self, content: str, source: str = "unknown"):
        """Store a fact with provenance (source + timestamp) and embedding."""
        emb = self.embedder.encode([content], convert_to_numpy=True)[0]
        entry = {
            "content": content,
            "source": source,
            "time": datetime.now().isoformat(timespec="seconds"),
            "embedding": emb,
        }
        self.knowledge.append(entry)
        return entry

    def search_facts(self, query: str, top_k: int = 3):
        """Retrieve top-k facts by cosine similarity with provenance included."""
        if not self.knowledge:
            return []

        q_emb = self.embedder.encode([query], convert_to_numpy=True)[0]
        mats = np.stack([e["embedding"] for e in self.knowledge], axis=0)  # (N, D)

        sims = (mats @ q_emb) / (np.linalg.norm(mats, axis=1) * (np.linalg.norm(q_emb) + 1e-12))
        idxs = np.argsort(-sims)[:top_k]

        results = []
        for i in idxs:
            results.append({
                "content": self.knowledge[i]["content"],
                "source": self.knowledge[i]["source"],
                "time": self.knowledge[i]["time"],
                "score": float(sims[i])
            })
        return results

# -----------------------------
# Research Agent
# -----------------------------
class ResearchAgent:
    def __init__(self, memory_agent):
        self.memory = memory_agent

    def run(self, query: str):
        # Get results from memory
        results = self.memory.search_facts(query, top_k=3)
        if results:
            # Already standardized with "content"
            return results

        # Fallback default knowledge (dicts with "content")
        defaults = [
            "Gradient Descent is a simple optimization method; it is widely known but can converge slowly on complex surfaces.",
            "Stochastic Gradient Descent (SGD) uses noisy updates and can scale to large datasets; momentum often helps.",
            "Adam uses adaptive learning rates per-parameter and often converges faster on neural networks.",
            "RMSProp adapts learning rates and is useful for recurrent networks or non-stationary objectives."
        ]
        return [
            {"content": d, "source": "ResearchAgent", "time": ""}
            for d in defaults
        ]





# -----------------------------
# Analysis Agent
# -----------------------------
class AnalysisAgent:
    def run(self, query: str) -> str:
        query = query.lower()

        if "optimizer" in query or "gradient descent" in query:
            return (
                "- Gradient Descent: Simple, widely used, but can be slow.\n"
                "- Stochastic Gradient Descent (SGD): Faster, adds randomness, may escape local minima.\n"
                "- Adam: Adaptive learning rates, usually converges faster, very popular."
            )

        return "Sorry, I don't have enough info to compare that yet."



# -----------------------------
# Fallback Classifier (BERT-based)
# -----------------------------
class FallbackClassifier:
    def __init__(self):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.labels = {
            "research": ["what is", "tell me about", "explain", "define", "name", "list"],
            "analysis": ["compare", "difference", "advantages", "disadvantages", "optimizer"],
            "memory": ["remember", "store", "save", "note", "recall"],
        }
        self.label_embeddings = {
            key: self.model.encode(values, convert_to_tensor=True)
            for key, values in self.labels.items()
        }

    def classify(self, query: str):
        query_emb = self.model.encode(query, convert_to_tensor=True)
        best_label, best_score = None, -1
        for label, embs in self.label_embeddings.items():
            sim = util.cos_sim(query_emb, embs).max().item()
            if sim > best_score:
                best_label, best_score = label, sim
        return best_label if best_score > 0.5 else "fallback"


#---------------------------------
#Summarizer Agent
#-------------------------------
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from typing import List

class SummarizerAgent:
    def __init__(self, model_name="t5-small", device="cpu"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

    def run(self, texts: List[str]) -> str:
        # Merge all texts into one chunk
        combined = " ".join(texts)
        combined = combined[:1000]  # safety cutoff

        input_length = len(combined.split())
        if input_length < 50:
            return combined

        # Dynamically set new tokens (instead of max_length)
        max_new = max(20, min(120, int(input_length * 0.7)))

        inputs = self.tokenizer(combined, return_tensors="pt", truncation=True).to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new,
            do_sample=False
        )
        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return summary

# -----------------------------
# Router (rule-based + fallback)
# -----------------------------
import numpy as np

class Router:
    def __init__(self, embedder, threshold: float = 0.35):
        self.embedder = embedder
        self.threshold = threshold

        # Rule-based keyword mapping
        self.rules = {
            "research": ["what is", "find", "tell me about", "name", "list"],
            "analysis": ["compare", "which is better", "which is best", "vs", "difference"],
            "memory": ["remember", "recall", "what did we learn", "note"],
            "greet": ["hello", "hi", "hey", "good morning", "good evening"],
        }

    def route(self, query: str) -> str:
        """Rule-based intent detection first."""
        q = query.lower()
        for intent, keywords in self.rules.items():
            if any(k in q for k in keywords):
                return intent
        return "fallback"

    def classify_with_fallback(self, query: str) -> str:
        """Semantic fallback using embeddings if rule-based routing fails."""
        labels = list(self.rules.keys())  # ["research", "analysis", "memory", "greet"]
        q_emb = self.embedder.encode([query], convert_to_numpy=True)[0]
        label_embs = self.embedder.encode(labels, convert_to_numpy=True)

        # cosine similarity
        sims = (label_embs @ q_emb) / (
            np.linalg.norm(label_embs, axis=1) * (np.linalg.norm(q_emb) + 1e-12)
        )
        best_idx = int(np.argmax(sims))
        best_score = float(sims[best_idx])

        if best_score >= self.threshold:
            return labels[best_idx]
        return "fallback"

# -----------------------------
# Coordinator
# -----------------------------
import datetime
from datetime import datetime


class Coordinator:
    def __init__(self, memory, research, analysis, router, summarizer=None):
        self.memory = memory
        self.research = research
        self.analysis = analysis
        self.router = router
        self.summarizer = summarizer

        self.conversation_log = []  # for show conversation
        self.trace_log = []         # for show trace

    # --- Conversation Logging ---
    def log_exchange(self, speaker, text):
        self.conversation_log.append({
            "time": datetime.now().isoformat(timespec="seconds"),
            "speaker": speaker,
            "text": text
        })

    def show_conversation(self):
        if not self.conversation_log:
            return "No conversation yet."
        lines = []
        for entry in self.conversation_log:
            t = entry["time"].split("T")[-1]
            lines.append(f"[{t}] {entry['speaker']}: {entry['text']}")
        return "\n".join(lines)

    # --- Trace Logging ---
    def log_trace(self, agent, action, detail=""):
        self.trace_log.append({
            "time": datetime.now().isoformat(timespec="seconds"),
            "agent": agent,
            "action": action,
            "detail": detail
        })

    def show_trace(self):
        if not self.trace_log:
            return "No trace yet."
        lines = []
        for entry in self.trace_log:
            t = entry["time"].split("T")[-1]
            lines.append(f"[{t}] {entry['agent']} â†’ {entry['action']} {entry['detail']}")
        return "\n".join(lines)

    # --- Main Handler ---
    def handle(self, user_input: str) -> str:
        # --- Logging user input ---
        self.log_exchange("You", user_input)

        # --- Special commands ---
        if user_input.lower() in ["show conversation", "conversation log"]:
            response = self.show_conversation()
            self.log_exchange("Agent", response)
            return response

        if user_input.lower() in ["show trace", "trace log"]:
            response = self.show_trace()
            self.log_exchange("Agent", response)
            return response

        # --- Step 1: Intent Routing ---
        intent = self.router.route(user_input)
        self.log_trace("Router", "classified intent", intent)

        if intent == "fallback":
            intent = self.router.classify_with_fallback(user_input)
            self.log_trace("Router", "fallback classification", intent)

        # --- Step 2: Dispatch ---
        if intent == "research":
            results = self.research.run(user_input)
            self.log_trace("ResearchAgent", "retrieved facts", f"{len(results)} found")

            for r in results:
                self.memory.store_fact(r["content"], source="ResearchAgent")
            self.log_trace("Coordinator", "stored facts", f"{len(results)} added to memory")

            response = "Hereâ€™s what I found:\n" + "\n".join(f"- {r['content']}" for r in results)

        elif intent == "analysis":
            result = self.analysis.run(user_input)
            self.memory.store_fact(result, source="AnalysisAgent")
            self.log_trace("AnalysisAgent", "completed analysis")
            self.log_trace("Coordinator", "stored analysis", "1 entry")

            response = "Hereâ€™s my analysis:\n" + result

        elif intent == "memory":
            facts = self.memory.search_facts(user_input, top_k=3)
            self.log_trace("MemoryAgent", "search", f"{len(facts)} results")

            if not facts:
                response = "I don't recall anything relevant."
            else:
                lines = []
                for f in facts:
                    t = f["time"].split("T")[-1][:5]
                    lines.append(f"- {f['content']}  ({f['source']}, {t})")
                response = "Hereâ€™s what I recall:\n" + "\n".join(lines)

        elif intent == "greet":
            self.log_trace("Coordinator", "handled greet")
            response = "Hello! How can I help you today?"

        else:
            self.log_trace("Coordinator", "intent not recognized")
            response = "Sorry â€” I couldn't understand the request. Try phrasing as a question (e.g., 'compare optimizers' or 'name some optimizers')."

        # --- Logging agent response ---
        self.log_exchange("Agent", response)
        return response

# -----------------------------
# Bootstrap with pre-loaded knowledge
# -----------------------------
def preload_knowledge(memory):
    defaults = [
        "Gradient Descent is a simple optimization method; it is widely known but can converge slowly on complex surfaces.",
        "Stochastic Gradient Descent (SGD) uses noisy updates and can scale to large datasets; momentum often helps.",
        "Adam uses adaptive learning rates per-parameter and often converges faster on neural networks.",
        "RMSProp adapts learning rates and is useful for recurrent networks or non-stationary objectives."
    ]
    for fact in defaults:
        memory.store_fact(fact, source="bootstrap")

# -----------------------------
# Example REPL
# -----------------------------
if __name__ == "__main__":
    from sentence_transformers import SentenceTransformer

    print("Initializing models (this may take a moment)...")
    embedder = SentenceTransformer("all-MiniLM-L6-v2")

    # Agents
    memory_agent = MemoryAgent(embedder)
    preload_knowledge(memory_agent)  # ðŸ‘ˆ preload defaults

    research_agent = ResearchAgent(memory_agent)
    analysis_agent = AnalysisAgent()
    router = Router(embedder)
    summarizer_agent = SummarizerAgent("t5-small")

    # Coordinator
    assistant = Coordinator(memory_agent, research_agent, analysis_agent, router, summarizer_agent)


    # Startup message
    print("\nAssistant ready. Type 'quit' to exit.")
    print("Available commands:")
    print(" - name optimizers")
    print(" - compare optimizers")
    print(" - recall what we learnt")
    print(" - show conversation\n")

    # REPL loop
    while True:
        query = input("You: ")
        if query.lower() in ["quit", "exit"]:
            print("Goodbye!")
            break
        response = assistant.handle(query)
        print(f"Agent: {response}\n")

