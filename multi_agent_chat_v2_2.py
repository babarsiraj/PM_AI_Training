# -*- coding: utf-8 -*-
"""multi_agent_chat_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0I0i7nRkE5zW1g-q33tzFFrnCyVUTb2
"""

!pip install groq

# -*- coding: utf-8 -*-
"""multi_agent_chat_v2_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqFM4v6PW4xEemhOFb8frtZ3tLgn9ti1

acting on first trigger word like tell me, name not going beyond that
show conversation and trace working
recall same problem
"""

import datetime
import numpy as np
# hybrid_assistant.py
from datetime import datetime

from typing import List, Dict
from sentence_transformers import SentenceTransformer, util

# multi_agent_chat_v2_2_groq_bootstrap.py

from sentence_transformers import SentenceTransformer
from typing import List, Dict
from datetime import datetime
import numpy as np
import os

# -----------------------------
# MemoryAgent
# -----------------------------
class MemoryAgent:
    def __init__(self, embedder):
        self.embedder = embedder
        self.facts = []

    def store_fact(self, content: str, source: str):
        emb = self.embedder.encode([content], convert_to_numpy=True)[0]
        self.facts.append({
            "content": content,
            "source": source,
            "time": datetime.now().isoformat(timespec="seconds"),
            "embedding": emb
        })

    def search_facts(self, query: str, top_k: int = 3) -> List[Dict]:
        if not self.facts:
            return []
        q_emb = self.embedder.encode([query], convert_to_numpy=True)[0]
        sims = [np.dot(q_emb, f["embedding"]) /
                (np.linalg.norm(q_emb) * np.linalg.norm(f["embedding"]))
                for f in self.facts]
        top_idx = np.argsort(sims)[::-1][:top_k]
        return [{"content": self.facts[i]["content"],
                 "source": self.facts[i]["source"],
                 "time": self.facts[i]["time"],
                 "score": float(sims[i])}
                for i in top_idx]

# -----------------------------
# ResearchAgent
# -----------------------------
class ResearchAgent:
    def __init__(self, memory: MemoryAgent):
        self.memory = memory

    def run(self, query: str) -> List[Dict]:
        results = self.memory.search_facts(query, top_k=3)
        return results

# -----------------------------
# AnalysisAgent
# -----------------------------
class AnalysisAgent:
    def run(self, query: str) -> List[str]:
        q = query.lower()
        if "optimizer" in q or "gradient descent" in q:
            return [
                "Gradient Descent: Simple, widely used, but can be slow.",
                "Stochastic Gradient Descent (SGD): Faster, adds randomness, may escape local minima.",
                "Adam: Adaptive learning rates, usually converges faster, very popular."
            ]
        return ["Sorry, I don't have enough info to compare that yet."]

# -----------------------------
# Router
# -----------------------------
class Router:
    def __init__(self):
        self.rules = {
            "research": ["name", "list", "find", "what is", "tell me", "describe", "explain"],
            "analysis": ["compare", "better", "best", "versus"],
            "memory": ["recall", "remember"],
            "greet": ["hello", "hi", "hey"],
            "trace": ["show trace"],
            "conversation": ["show conversation"],
        }

    def route(self, query: str) -> str:
        q = query.lower()
        for intent, keywords in self.rules.items():
            if any(k in q for k in keywords):
                return intent
        return "fallback"

    def classify_with_fallback(self, query: str) -> str:
        return "llm"

# -----------------------------
# SummarizerAgent (stub – can use T5 if needed)
# -----------------------------
class SummarizerAgent:
    def run(self, texts: List[str]) -> str:
        return " ".join(texts[:2])  # naive shortener

# -----------------------------
# Coordinator
# -----------------------------
class Coordinator:
    def __init__(self, memory, research, analysis, router, summarizer=None, groq_client=None):
        self.memory = memory
        self.research = research
        self.analysis = analysis
        self.router = router
        self.summarizer = summarizer
        self.groq_client = groq_client

        self.trace = []
        self.conversation_log = []

    # --- Logging ---
    def log_trace(self, agent: str, action: str, detail: str):
        self.trace.append(
            (datetime.now().isoformat(timespec="seconds"), agent, action, detail)
        )

    def log_exchange(self, speaker: str, text: str):
        self.conversation_log.append({
            "time": datetime.now().isoformat(timespec="seconds"),
            "speaker": speaker,
            "text": text
        })

    def show_trace(self) -> str:
        if not self.trace:
            return "No trace yet."
        return "\n".join([f"[{t}] {a} → {act}: {d}" for t,a,act,d in self.trace])

    def show_conversation(self) -> str:
        if not self.conversation_log:
            return "No conversation yet."
        return "\n".join([f"[{c['time']}] {c['speaker']}: {c['text']}" for c in self.conversation_log])

    # --- Groq helper ---
    def call_groq(self, query: str) -> str:
        try:
            resp = self.groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": query}],
                temperature=0,
                max_tokens=200
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            return f"[Groq error] {e}"

    # --- Main handler ---
    def handle(self, user_input: str) -> str:
        self.log_exchange("You", user_input)

        intent = self.router.route(user_input)
        if intent == "fallback":
            intent = self.router.classify_with_fallback(user_input)
        self.log_trace("Router", "classified", intent)

        response = ""
        if intent == "greet":
            response = "Hello! How can I help you today?"

        elif intent == "research":
            results = self.research.run(user_input)
            for r in results:
                self.memory.store_fact(r["content"], source="ResearchAgent")
            response = "Here’s what I found:\n" + "\n".join(f"- {r['content']}" for r in results)

        elif intent == "analysis":
            results = self.analysis.run(user_input)
            for r in results:
                self.memory.store_fact(r, source="AnalysisAgent")
            response = "Here’s my analysis:\n" + "\n".join(f"- {r}" for r in results)

        elif intent == "memory":
            results = self.memory.search_facts(user_input, top_k=3)
            if results:
                response = "Here’s what I recall:\n" + "\n".join(
                    f"- {r['content']}  ({r['source']}, {r['time']})"
                    for r in results
                )
            else:
                response = "I couldn’t find anything relevant in memory."

        elif intent == "trace":
            response = self.show_trace()

        elif intent == "conversation":
            response = self.show_conversation()

        elif intent == "llm":
            if self.groq_client:
                response = self.call_groq(user_input)
                self.memory.store_fact(response, source="Groq")
            else:
                response = "Groq is not configured; cannot call LLM."

        else:
            response = "Sorry, I couldn’t understand."

        self.log_exchange("Agent", response)
        return response

# -----------------------------
# Bootstrap knowledge
# -----------------------------
def preload_knowledge(memory: MemoryAgent):
    bootstrap_facts = [
        "Gradient Descent is a simple optimization method; it is widely known but can converge slowly.",
        "Stochastic Gradient Descent (SGD) uses noisy updates and can scale to large datasets; momentum often helps.",
        "Adam uses adaptive learning rates per-parameter and often converges faster on neural networks.",
        "RMSProp adapts learning rates and is useful for recurrent networks or non-stationary objectives.",
        "Transformers in AI are a neural network architecture using self-attention for sequential data.",
        "The attention mechanism in transformers lets the model focus on relevant parts of the input sequence.",
        "ReLU is a widely used activation function in deep learning, allowing faster training convergence.",
        "Dropout is a regularization method to prevent overfitting in neural networks.",
        "Batch Normalization helps stabilize training by normalizing layer inputs."
    ]
    for fact in bootstrap_facts:
        memory.store_fact(fact, source="bootstrap")

# -----------------------------
# Example REPL
# -----------------------------
if __name__ == "__main__":
    print("Initializing models (this may take a moment)...")
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    memory_agent = MemoryAgent(embedder)
    preload_knowledge(memory_agent)   # ✅ preload default knowledge
    research_agent = ResearchAgent(memory_agent)
    analysis_agent = AnalysisAgent()
    router = Router()
    summarizer_agent = SummarizerAgent()

    # --- Setup Groq ---
    GROQ_API_KEY = "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"
    groq_client = None
    if GROQ_API_KEY:
        try:
            from groq import Groq
            groq_client = Groq(api_key=GROQ_API_KEY)
            print("[Startup] ✅ Groq client configured.")
        except Exception as e:
            print(f"[Startup] ⚠️ Groq not usable: {e}")
    else:
        print("[Startup] ℹ️ No Groq API key; running local only.")

    assistant = Coordinator(memory_agent, research_agent, analysis_agent, router, summarizer_agent, groq_client)

    print("\nAssistant ready. Type 'quit' to exit.\n")

    while True:
        query = input("You: ")
        if query.lower() in ["quit", "exit"]:
            print("Goodbye!")
            break
        response = assistant.handle(query)
        print(f"Agent: {response}\n")